{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_onebyone(\n",
      "  (word_embeddings): Embedding(5666, 300)\n",
      "  (lstm): LSTM(300, 64)\n",
      "  (hidden2label): Linear(in_features=128, out_features=1)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WANG\\Downloads\\nn4nlp\\nn4nlp\\utils.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(torch.cat([qa, qb, qc, qd], 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0/100] Training loss: 0.044, Testing loss: 0.049, Training acc: 0.233, Testing acc: 0.255\n",
      "[Epoch   1/100] Training loss: 0.044, Testing loss: 0.049, Training acc: 0.263, Testing acc: 0.255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3394f17bae80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m             \u001b[0mquestions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataLoaderIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdaemon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m  \u001b[1;31m# ensure that the worker exits on process exit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m                 \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\process.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    103\u001b[0m                \u001b[1;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36m_Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpopen_spawn_win32\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0mSpawnContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mreduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_child\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mset_spawning_popen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;34m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mForkingPickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import LSTM_basic, QA_Dataset, LSTM_onebyone\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "torch.manual_seed(11)\n",
    "corpus_path = './corpus.pkl'\n",
    "embedding_path = './pretrained_embed.npy'\n",
    "save_model = False\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "\n",
    "def load_corpus(corpus_path):\n",
    "    with open(corpus_path, 'rb') as handle:\n",
    "        corpus = pickle.load(handle)\n",
    "    return corpus\n",
    "\n",
    "def load_embeddings(embed_path):\n",
    "    return np.load(embed_path)\n",
    "\n",
    "def save_checkpoint(state, fname):\n",
    "    print(\"Save model to %s\"%fname)\n",
    "    torch.save(state, fname)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = load_corpus(corpus_path)\n",
    "    embeddings = load_embeddings(embedding_path)\n",
    "    # word2id = {y:x for x, y in corpus.items()}\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    # use_cuda = False\n",
    "    model = LSTM_onebyone(embedding_dim = embedding_dim, hidden_dim = hidden_dim, vocab_size = len(corpus),\\\n",
    "                       batch_size = batch_size, use_cuda = use_cuda, embeddings = embeddings)\n",
    "    print(model)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    data_dir = './ready_data_onebyone'\n",
    "    ftrain = 'mc500.train.tsv'\n",
    "    fdev = 'mc500.dev.tsv'\n",
    "\n",
    "    train_set = QA_Dataset(data_dir, ftrain, corpus, onebyone = True)\n",
    "    dev_set = QA_Dataset(data_dir, fdev, corpus, onebyone = True)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "    dev_loader = DataLoader(dev_set, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "    \n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = learning_rate, weight_decay=1e-2)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    train_loss_ = []\n",
    "    test_loss_ = []\n",
    "    train_acc_ = []\n",
    "    test_acc_ = []\n",
    "\n",
    "    rawQuestions, rawAnswers, rawLabels, rawOutput = [], [], [], []\n",
    "\n",
    "    # Training \n",
    "    for epoch in range(num_epochs):\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0\n",
    "        for iter, data in enumerate(train_loader):\n",
    "            questions, answers, labels, _, _, _ = data\n",
    "            labels = torch.squeeze(labels)\n",
    "            l = labels\n",
    "            if use_cuda:\n",
    "                q, l = Variable(questions.cuda()).t(), labels.cuda()\n",
    "                a = Variable(answers[0].cuda()).t()\n",
    "                b = Variable(answers[1].cuda()).t()\n",
    "                c = Variable(answers[2].cuda()).t()\n",
    "                d = Variable(answers[3].cuda()).t()\n",
    "            else:\n",
    "                q = Variable(questions).t()\n",
    "                a = Variable(answers[0]).t()\n",
    "                b = Variable(answers[1]).t()\n",
    "                c = Variable(answers[2]).t()\n",
    "                d = Variable(answers[3]).t()\n",
    "            model.batch_size = len(labels)\n",
    "            output = model(q, a, b, c, d)\n",
    "            loss = loss_function(output.cpu(), Variable(l).cpu())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculating training accuracy\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            total_acc += (pred.cpu() == labels).sum()\n",
    "            total += len(labels)\n",
    "            total_loss += loss.data[0]\n",
    "        train_loss_.append(total_loss / total)\n",
    "        train_acc_.append(total_acc / total)\n",
    "        if save_model:\n",
    "            fname = '{}/Epoch_{}.model'.format('./checkpoints', epoch)\n",
    "            save_checkpoint({'epoch': epoch + 1,\n",
    "                             'state_dict': model.state_dict(),\n",
    "                             'optimizer': optimizer.state_dict()},\n",
    "                           fname = fname)\n",
    "\n",
    "        # Testing/Validating\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0\n",
    "        for iter, data in enumerate(dev_loader):\n",
    "            questions, answers, labels, origQuestions, origAnswers, origLabels = data\n",
    "            labels = torch.squeeze(labels)\n",
    "            l = labels\n",
    "            if use_cuda:\n",
    "                q, l = Variable(questions.cuda()).t(), labels.cuda()\n",
    "                a = Variable(answers[0].cuda()).t()\n",
    "                b = Variable(answers[1].cuda()).t()\n",
    "                c = Variable(answers[2].cuda()).t()\n",
    "                d = Variable(answers[3].cuda()).t()\n",
    "            else:\n",
    "                q = Variable(questions).t()\n",
    "                a = Variable(answers[0]).t()\n",
    "                b = Variable(answers[1]).t()\n",
    "                c = Variable(answers[2]).t()\n",
    "                d = Variable(answers[3]).t()\n",
    "\n",
    "            model.batch_size = len(labels)\n",
    "            output = model(q, a, b, c, d)\n",
    "            loss = loss_function(output.cpu(), Variable(labels))\n",
    "            if epoch == num_epochs - 1:\n",
    "                rawQuestions.append(origQuestions)\n",
    "                rawAnswers.append(origAnswers)\n",
    "                rawLabels.append(origLabels)\n",
    "                rawOutput.append(output.data.cpu().numpy())\n",
    "\n",
    "            # Calculating training accuracy\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            total_acc += (pred.cpu() == labels).sum()\n",
    "            total += len(labels)\n",
    "            total_loss += loss.data[0]\n",
    "        test_loss_.append(total_loss / total)\n",
    "        test_acc_.append(total_acc / total)\n",
    "        \n",
    "        print(\"[Epoch %3d/%3d] Training loss: %.3f, Testing loss: %.3f, Training acc: %.3f, Testing acc: %.3f\"%\\\n",
    "              (epoch, num_epochs, train_loss_[epoch], test_loss_[epoch],\n",
    "                                 train_acc_[epoch], test_acc_[epoch]))\n",
    "\n",
    "    correct, wrong = [], []\n",
    "    for i in range(len(rawLabels)):\n",
    "        for j in range(len(rawOutput[i])):\n",
    "            # print(rawLabels[i][j], rawOutput[i][j])\n",
    "            choice = int(np.argmax(rawOutput[i][j]))\n",
    "            if rawLabels[i][j] == choice:\n",
    "                correct.append((rawQuestions[i][j], rawAnswers[i][j], rawLabels[i][j], choice))\n",
    "            else:\n",
    "                wrong.append((rawQuestions[i][j], rawAnswers[i][j], rawLabels[i][j], choice))\n",
    "    \n",
    "    with open(\"correct_500.json\", \"w\") as f:\n",
    "        json.dump(correct, f)\n",
    "\n",
    "    with open(\"wrong_500.json\", \"w\") as f:\n",
    "        json.dump(wrong, f)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_onebyone(\n",
      "  (word_embeddings): Embedding(5666, 300)\n",
      "  (lstm): LSTM(300, 64)\n",
      "  (hidden2label): Linear(in_features=128, out_features=1)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WANG\\Downloads\\nn4nlp\\nn4nlp\\utils.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(torch.cat([qa, qb, qc, qd], 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0/100] Training loss: 0.087, Testing loss: 0.090, Training acc: 0.226, Testing acc: 0.275\n",
      "[Epoch   1/100] Training loss: 0.086, Testing loss: 0.090, Training acc: 0.267, Testing acc: 0.255\n",
      "[Epoch   2/100] Training loss: 0.086, Testing loss: 0.091, Training acc: 0.266, Testing acc: 0.270\n",
      "[Epoch   3/100] Training loss: 0.087, Testing loss: 0.090, Training acc: 0.247, Testing acc: 0.265\n",
      "[Epoch   4/100] Training loss: 0.086, Testing loss: 0.090, Training acc: 0.257, Testing acc: 0.260\n",
      "[Epoch   5/100] Training loss: 0.086, Testing loss: 0.091, Training acc: 0.274, Testing acc: 0.270\n",
      "[Epoch   6/100] Training loss: 0.085, Testing loss: 0.091, Training acc: 0.292, Testing acc: 0.275\n",
      "[Epoch   7/100] Training loss: 0.084, Testing loss: 0.091, Training acc: 0.310, Testing acc: 0.270\n",
      "[Epoch   8/100] Training loss: 0.083, Testing loss: 0.091, Training acc: 0.320, Testing acc: 0.260\n",
      "[Epoch   9/100] Training loss: 0.083, Testing loss: 0.091, Training acc: 0.327, Testing acc: 0.275\n",
      "[Epoch  10/100] Training loss: 0.083, Testing loss: 0.091, Training acc: 0.329, Testing acc: 0.280\n",
      "[Epoch  11/100] Training loss: 0.082, Testing loss: 0.090, Training acc: 0.340, Testing acc: 0.285\n",
      "[Epoch  12/100] Training loss: 0.082, Testing loss: 0.090, Training acc: 0.330, Testing acc: 0.290\n",
      "[Epoch  13/100] Training loss: 0.082, Testing loss: 0.092, Training acc: 0.340, Testing acc: 0.255\n",
      "[Epoch  14/100] Training loss: 0.082, Testing loss: 0.091, Training acc: 0.347, Testing acc: 0.285\n",
      "[Epoch  15/100] Training loss: 0.081, Testing loss: 0.091, Training acc: 0.357, Testing acc: 0.285\n",
      "[Epoch  16/100] Training loss: 0.082, Testing loss: 0.092, Training acc: 0.351, Testing acc: 0.255\n",
      "[Epoch  17/100] Training loss: 0.081, Testing loss: 0.091, Training acc: 0.353, Testing acc: 0.275\n",
      "[Epoch  18/100] Training loss: 0.081, Testing loss: 0.091, Training acc: 0.352, Testing acc: 0.285\n",
      "[Epoch  19/100] Training loss: 0.081, Testing loss: 0.091, Training acc: 0.346, Testing acc: 0.260\n",
      "[Epoch  20/100] Training loss: 0.081, Testing loss: 0.092, Training acc: 0.358, Testing acc: 0.255\n",
      "[Epoch  21/100] Training loss: 0.081, Testing loss: 0.092, Training acc: 0.362, Testing acc: 0.260\n",
      "[Epoch  22/100] Training loss: 0.081, Testing loss: 0.093, Training acc: 0.356, Testing acc: 0.240\n",
      "[Epoch  23/100] Training loss: 0.080, Testing loss: 0.092, Training acc: 0.365, Testing acc: 0.255\n",
      "[Epoch  24/100] Training loss: 0.080, Testing loss: 0.092, Training acc: 0.373, Testing acc: 0.260\n",
      "[Epoch  25/100] Training loss: 0.079, Testing loss: 0.092, Training acc: 0.378, Testing acc: 0.260\n",
      "[Epoch  26/100] Training loss: 0.080, Testing loss: 0.092, Training acc: 0.377, Testing acc: 0.260\n",
      "[Epoch  27/100] Training loss: 0.080, Testing loss: 0.092, Training acc: 0.375, Testing acc: 0.270\n",
      "[Epoch  28/100] Training loss: 0.080, Testing loss: 0.091, Training acc: 0.369, Testing acc: 0.275\n",
      "[Epoch  29/100] Training loss: 0.080, Testing loss: 0.093, Training acc: 0.372, Testing acc: 0.250\n",
      "[Epoch  30/100] Training loss: 0.080, Testing loss: 0.093, Training acc: 0.376, Testing acc: 0.245\n",
      "[Epoch  31/100] Training loss: 0.079, Testing loss: 0.093, Training acc: 0.375, Testing acc: 0.250\n",
      "[Epoch  32/100] Training loss: 0.079, Testing loss: 0.092, Training acc: 0.386, Testing acc: 0.240\n",
      "[Epoch  33/100] Training loss: 0.079, Testing loss: 0.093, Training acc: 0.383, Testing acc: 0.260\n",
      "[Epoch  34/100] Training loss: 0.079, Testing loss: 0.093, Training acc: 0.379, Testing acc: 0.250\n",
      "[Epoch  35/100] Training loss: 0.079, Testing loss: 0.093, Training acc: 0.390, Testing acc: 0.250\n",
      "[Epoch  36/100] Training loss: 0.080, Testing loss: 0.092, Training acc: 0.380, Testing acc: 0.250\n",
      "[Epoch  37/100] Training loss: 0.079, Testing loss: 0.092, Training acc: 0.382, Testing acc: 0.265\n",
      "[Epoch  38/100] Training loss: 0.079, Testing loss: 0.093, Training acc: 0.387, Testing acc: 0.245\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c27811175c42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\nn4nlp\\nn4nlp\\utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, question, a, b, c, d)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0membed_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mout_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[0mout_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mout_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         )\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, weight, hidden)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mnexth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, hidden, weight)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0mhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, hidden, weight)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[1;31m# hack to handle LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[1;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mgates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mingate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import LSTM_basic, QA_Dataset, LSTM_onebyone\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "torch.manual_seed(11)\n",
    "corpus_path = './corpus.pkl'\n",
    "embedding_path = './pretrained_embed.npy'\n",
    "save_model = False\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "\n",
    "def load_corpus(corpus_path):\n",
    "    with open(corpus_path, 'rb') as handle:\n",
    "        corpus = pickle.load(handle)\n",
    "    return corpus\n",
    "\n",
    "def load_embeddings(embed_path):\n",
    "    return np.load(embed_path)\n",
    "\n",
    "def save_checkpoint(state, fname):\n",
    "    print(\"Save model to %s\"%fname)\n",
    "    torch.save(state, fname)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = load_corpus(corpus_path)\n",
    "    embeddings = load_embeddings(embedding_path)\n",
    "    # word2id = {y:x for x, y in corpus.items()}\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    # use_cuda = False\n",
    "    model = LSTM_onebyone(embedding_dim = embedding_dim, hidden_dim = hidden_dim, vocab_size = len(corpus),\\\n",
    "                       batch_size = batch_size, use_cuda = use_cuda, embeddings = embeddings)\n",
    "    print(model)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    data_dir = './ready_data_onebyone'\n",
    "    ftrain = 'mc500.train.tsv'\n",
    "    fdev = 'mc500.dev.tsv'\n",
    "\n",
    "    train_set = QA_Dataset(data_dir, ftrain, corpus, onebyone = True)\n",
    "    dev_set = QA_Dataset(data_dir, fdev, corpus, onebyone = True)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "    dev_loader = DataLoader(dev_set, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "    \n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = learning_rate, weight_decay=1e-2)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    train_loss_ = []\n",
    "    test_loss_ = []\n",
    "    train_acc_ = []\n",
    "    test_acc_ = []\n",
    "\n",
    "    rawQuestions, rawAnswers, rawLabels, rawOutput = [], [], [], []\n",
    "\n",
    "    # Training \n",
    "    for epoch in range(num_epochs):\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0\n",
    "        for iter, data in enumerate(train_loader):\n",
    "            questions, answers, labels, _, _, _ = data\n",
    "            labels = torch.squeeze(labels)\n",
    "            l = labels\n",
    "            if use_cuda:\n",
    "                q, l = Variable(questions.cuda()).t(), labels.cuda()\n",
    "                a = Variable(answers[0].cuda()).t()\n",
    "                b = Variable(answers[1].cuda()).t()\n",
    "                c = Variable(answers[2].cuda()).t()\n",
    "                d = Variable(answers[3].cuda()).t()\n",
    "            else:\n",
    "                q = Variable(questions).t()\n",
    "                a = Variable(answers[0]).t()\n",
    "                b = Variable(answers[1]).t()\n",
    "                c = Variable(answers[2]).t()\n",
    "                d = Variable(answers[3]).t()\n",
    "            model.batch_size = len(labels)\n",
    "            output = model(q, a, b, c, d)\n",
    "            loss = loss_function(output.cpu(), Variable(l).cpu())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculating training accuracy\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            total_acc += (pred.cpu() == labels).sum()\n",
    "            total += len(labels)\n",
    "            total_loss += loss.data[0]\n",
    "        train_loss_.append(total_loss / total)\n",
    "        train_acc_.append(total_acc / total)\n",
    "        if save_model:\n",
    "            fname = '{}/Epoch_{}.model'.format('./checkpoints', epoch)\n",
    "            save_checkpoint({'epoch': epoch + 1,\n",
    "                             'state_dict': model.state_dict(),\n",
    "                             'optimizer': optimizer.state_dict()},\n",
    "                           fname = fname)\n",
    "\n",
    "        # Testing/Validating\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0\n",
    "        for iter, data in enumerate(dev_loader):\n",
    "            questions, answers, labels, origQuestions, origAnswers, origLabels = data\n",
    "            labels = torch.squeeze(labels)\n",
    "            l = labels\n",
    "            if use_cuda:\n",
    "                q, l = Variable(questions.cuda()).t(), labels.cuda()\n",
    "                a = Variable(answers[0].cuda()).t()\n",
    "                b = Variable(answers[1].cuda()).t()\n",
    "                c = Variable(answers[2].cuda()).t()\n",
    "                d = Variable(answers[3].cuda()).t()\n",
    "            else:\n",
    "                q = Variable(questions).t()\n",
    "                a = Variable(answers[0]).t()\n",
    "                b = Variable(answers[1]).t()\n",
    "                c = Variable(answers[2]).t()\n",
    "                d = Variable(answers[3]).t()\n",
    "\n",
    "            model.batch_size = len(labels)\n",
    "            output = model(q, a, b, c, d)\n",
    "            loss = loss_function(output.cpu(), Variable(labels))\n",
    "            if epoch == num_epochs - 1:\n",
    "                rawQuestions.append(origQuestions)\n",
    "                rawAnswers.append(origAnswers)\n",
    "                rawLabels.append(origLabels)\n",
    "                rawOutput.append(output.data.cpu().numpy())\n",
    "\n",
    "            # Calculating training accuracy\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            total_acc += (pred.cpu() == labels).sum()\n",
    "            total += len(labels)\n",
    "            total_loss += loss.data[0]\n",
    "        test_loss_.append(total_loss / total)\n",
    "        test_acc_.append(total_acc / total)\n",
    "        \n",
    "        print(\"[Epoch %3d/%3d] Training loss: %.3f, Testing loss: %.3f, Training acc: %.3f, Testing acc: %.3f\"%\\\n",
    "              (epoch, num_epochs, train_loss_[epoch], test_loss_[epoch],\n",
    "                                 train_acc_[epoch], test_acc_[epoch]))\n",
    "\n",
    "    correct, wrong = [], []\n",
    "    for i in range(len(rawLabels)):\n",
    "        for j in range(len(rawOutput[i])):\n",
    "            # print(rawLabels[i][j], rawOutput[i][j])\n",
    "            choice = int(np.argmax(rawOutput[i][j]))\n",
    "            if rawLabels[i][j] == choice:\n",
    "                correct.append((rawQuestions[i][j], rawAnswers[i][j], rawLabels[i][j], choice))\n",
    "            else:\n",
    "                wrong.append((rawQuestions[i][j], rawAnswers[i][j], rawLabels[i][j], choice))\n",
    "    \n",
    "    with open(\"correct_500.json\", \"w\") as f:\n",
    "        json.dump(correct, f)\n",
    "\n",
    "    with open(\"wrong_500.json\", \"w\") as f:\n",
    "        json.dump(wrong, f)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_onebyone(\n",
      "  (word_embeddings): Embedding(5666, 300)\n",
      "  (lstm): LSTM(300, 256)\n",
      "  (hidden2label): Linear(in_features=512, out_features=1)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WANG\\Downloads\\nn4nlp\\nn4nlp\\utils.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(torch.cat([qa, qb, qc, qd], 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0/100] Training loss: 0.087, Testing loss: 0.090, Training acc: 0.233, Testing acc: 0.265\n",
      "[Epoch   1/100] Training loss: 0.087, Testing loss: 0.090, Training acc: 0.273, Testing acc: 0.265\n",
      "[Epoch   2/100] Training loss: 0.087, Testing loss: 0.090, Training acc: 0.275, Testing acc: 0.255\n",
      "[Epoch   3/100] Training loss: 0.086, Testing loss: 0.090, Training acc: 0.283, Testing acc: 0.245\n",
      "[Epoch   4/100] Training loss: 0.086, Testing loss: 0.090, Training acc: 0.296, Testing acc: 0.270\n",
      "[Epoch   5/100] Training loss: 0.085, Testing loss: 0.090, Training acc: 0.292, Testing acc: 0.245\n",
      "[Epoch   6/100] Training loss: 0.084, Testing loss: 0.090, Training acc: 0.308, Testing acc: 0.300\n",
      "[Epoch   7/100] Training loss: 0.084, Testing loss: 0.091, Training acc: 0.316, Testing acc: 0.285\n",
      "[Epoch   8/100] Training loss: 0.084, Testing loss: 0.090, Training acc: 0.329, Testing acc: 0.285\n",
      "[Epoch   9/100] Training loss: 0.083, Testing loss: 0.091, Training acc: 0.315, Testing acc: 0.270\n",
      "[Epoch  10/100] Training loss: 0.082, Testing loss: 0.091, Training acc: 0.343, Testing acc: 0.270\n",
      "[Epoch  11/100] Training loss: 0.081, Testing loss: 0.091, Training acc: 0.352, Testing acc: 0.260\n",
      "[Epoch  12/100] Training loss: 0.080, Testing loss: 0.090, Training acc: 0.369, Testing acc: 0.295\n",
      "[Epoch  13/100] Training loss: 0.080, Testing loss: 0.091, Training acc: 0.372, Testing acc: 0.270\n",
      "[Epoch  14/100] Training loss: 0.080, Testing loss: 0.091, Training acc: 0.381, Testing acc: 0.275\n",
      "[Epoch  15/100] Training loss: 0.079, Testing loss: 0.090, Training acc: 0.384, Testing acc: 0.280\n",
      "[Epoch  16/100] Training loss: 0.079, Testing loss: 0.091, Training acc: 0.388, Testing acc: 0.275\n",
      "[Epoch  17/100] Training loss: 0.079, Testing loss: 0.091, Training acc: 0.390, Testing acc: 0.275\n",
      "[Epoch  18/100] Training loss: 0.079, Testing loss: 0.091, Training acc: 0.395, Testing acc: 0.280\n",
      "[Epoch  19/100] Training loss: 0.078, Testing loss: 0.092, Training acc: 0.398, Testing acc: 0.270\n",
      "[Epoch  20/100] Training loss: 0.078, Testing loss: 0.092, Training acc: 0.406, Testing acc: 0.265\n",
      "[Epoch  21/100] Training loss: 0.078, Testing loss: 0.092, Training acc: 0.405, Testing acc: 0.265\n",
      "[Epoch  22/100] Training loss: 0.078, Testing loss: 0.091, Training acc: 0.410, Testing acc: 0.285\n",
      "[Epoch  23/100] Training loss: 0.078, Testing loss: 0.091, Training acc: 0.407, Testing acc: 0.285\n",
      "[Epoch  24/100] Training loss: 0.077, Testing loss: 0.091, Training acc: 0.412, Testing acc: 0.275\n",
      "[Epoch  25/100] Training loss: 0.077, Testing loss: 0.091, Training acc: 0.416, Testing acc: 0.275\n",
      "[Epoch  26/100] Training loss: 0.077, Testing loss: 0.091, Training acc: 0.415, Testing acc: 0.275\n",
      "[Epoch  27/100] Training loss: 0.077, Testing loss: 0.091, Training acc: 0.424, Testing acc: 0.270\n",
      "[Epoch  28/100] Training loss: 0.077, Testing loss: 0.092, Training acc: 0.425, Testing acc: 0.265\n",
      "[Epoch  29/100] Training loss: 0.077, Testing loss: 0.091, Training acc: 0.424, Testing acc: 0.270\n",
      "[Epoch  30/100] Training loss: 0.077, Testing loss: 0.092, Training acc: 0.423, Testing acc: 0.275\n",
      "[Epoch  31/100] Training loss: 0.077, Testing loss: 0.092, Training acc: 0.426, Testing acc: 0.265\n",
      "[Epoch  32/100] Training loss: 0.076, Testing loss: 0.092, Training acc: 0.428, Testing acc: 0.265\n",
      "[Epoch  33/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.429, Testing acc: 0.275\n",
      "[Epoch  34/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.427, Testing acc: 0.280\n",
      "[Epoch  35/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.428, Testing acc: 0.275\n",
      "[Epoch  36/100] Training loss: 0.076, Testing loss: 0.092, Training acc: 0.430, Testing acc: 0.275\n",
      "[Epoch  37/100] Training loss: 0.076, Testing loss: 0.092, Training acc: 0.429, Testing acc: 0.260\n",
      "[Epoch  38/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.430, Testing acc: 0.285\n",
      "[Epoch  39/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.428, Testing acc: 0.285\n",
      "[Epoch  40/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.429, Testing acc: 0.285\n",
      "[Epoch  41/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.427, Testing acc: 0.285\n",
      "[Epoch  42/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.428, Testing acc: 0.290\n",
      "[Epoch  43/100] Training loss: 0.076, Testing loss: 0.090, Training acc: 0.431, Testing acc: 0.290\n",
      "[Epoch  44/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.427, Testing acc: 0.270\n",
      "[Epoch  45/100] Training loss: 0.077, Testing loss: 0.092, Training acc: 0.422, Testing acc: 0.265\n",
      "[Epoch  46/100] Training loss: 0.077, Testing loss: 0.091, Training acc: 0.426, Testing acc: 0.255\n",
      "[Epoch  47/100] Training loss: 0.077, Testing loss: 0.091, Training acc: 0.423, Testing acc: 0.275\n",
      "[Epoch  48/100] Training loss: 0.077, Testing loss: 0.090, Training acc: 0.417, Testing acc: 0.285\n",
      "[Epoch  49/100] Training loss: 0.077, Testing loss: 0.090, Training acc: 0.424, Testing acc: 0.290\n",
      "[Epoch  50/100] Training loss: 0.077, Testing loss: 0.091, Training acc: 0.427, Testing acc: 0.285\n",
      "[Epoch  51/100] Training loss: 0.077, Testing loss: 0.090, Training acc: 0.427, Testing acc: 0.290\n",
      "[Epoch  52/100] Training loss: 0.077, Testing loss: 0.091, Training acc: 0.425, Testing acc: 0.280\n",
      "[Epoch  53/100] Training loss: 0.077, Testing loss: 0.091, Training acc: 0.429, Testing acc: 0.280\n",
      "[Epoch  54/100] Training loss: 0.076, Testing loss: 0.092, Training acc: 0.432, Testing acc: 0.260\n",
      "[Epoch  55/100] Training loss: 0.077, Testing loss: 0.092, Training acc: 0.427, Testing acc: 0.265\n",
      "[Epoch  56/100] Training loss: 0.077, Testing loss: 0.091, Training acc: 0.427, Testing acc: 0.260\n",
      "[Epoch  57/100] Training loss: 0.076, Testing loss: 0.092, Training acc: 0.429, Testing acc: 0.265\n",
      "[Epoch  58/100] Training loss: 0.077, Testing loss: 0.092, Training acc: 0.427, Testing acc: 0.265\n",
      "[Epoch  59/100] Training loss: 0.076, Testing loss: 0.092, Training acc: 0.430, Testing acc: 0.265\n",
      "[Epoch  60/100] Training loss: 0.076, Testing loss: 0.092, Training acc: 0.427, Testing acc: 0.270\n",
      "[Epoch  61/100] Training loss: 0.076, Testing loss: 0.092, Training acc: 0.430, Testing acc: 0.265\n",
      "[Epoch  62/100] Training loss: 0.076, Testing loss: 0.092, Training acc: 0.434, Testing acc: 0.260\n",
      "[Epoch  63/100] Training loss: 0.076, Testing loss: 0.092, Training acc: 0.431, Testing acc: 0.265\n",
      "[Epoch  64/100] Training loss: 0.076, Testing loss: 0.092, Training acc: 0.432, Testing acc: 0.270\n",
      "[Epoch  65/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.436, Testing acc: 0.270\n",
      "[Epoch  66/100] Training loss: 0.076, Testing loss: 0.093, Training acc: 0.434, Testing acc: 0.270\n",
      "[Epoch  67/100] Training loss: 0.076, Testing loss: 0.092, Training acc: 0.433, Testing acc: 0.270\n",
      "[Epoch  68/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.431, Testing acc: 0.265\n",
      "[Epoch  69/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.429, Testing acc: 0.280\n",
      "[Epoch  70/100] Training loss: 0.076, Testing loss: 0.091, Training acc: 0.431, Testing acc: 0.285\n",
      "[Epoch  71/100] Training loss: 0.077, Testing loss: 0.091, Training acc: 0.427, Testing acc: 0.280\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-af7399a376a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\nn4nlp\\nn4nlp\\utils.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, question, a, b, c, d)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0membed_d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mout_q\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[0mout_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_a\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mout_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         )\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, weight, hidden)\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mnexth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, hidden, weight)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_directions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0mhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m                 \u001b[0mnext_hidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mall_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, hidden, weight)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreverse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m             \u001b[1;31m# hack to handle LSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mLSTMCell\u001b[1;34m(input, hidden, w_ih, w_hh, b_ih, b_hh)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mgates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_ih\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_ih\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_hh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_hh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mingate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforgetgate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcellgate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutgate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import LSTM_basic, QA_Dataset, LSTM_onebyone\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "torch.manual_seed(11)\n",
    "corpus_path = './corpus.pkl'\n",
    "embedding_path = './pretrained_embed.npy'\n",
    "save_model = False\n",
    "embedding_dim = 300\n",
    "hidden_dim = 256\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 100\n",
    "\n",
    "def load_corpus(corpus_path):\n",
    "    with open(corpus_path, 'rb') as handle:\n",
    "        corpus = pickle.load(handle)\n",
    "    return corpus\n",
    "\n",
    "def load_embeddings(embed_path):\n",
    "    return np.load(embed_path)\n",
    "\n",
    "def save_checkpoint(state, fname):\n",
    "    print(\"Save model to %s\"%fname)\n",
    "    torch.save(state, fname)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = load_corpus(corpus_path)\n",
    "    embeddings = load_embeddings(embedding_path)\n",
    "    # word2id = {y:x for x, y in corpus.items()}\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    # use_cuda = False\n",
    "    model = LSTM_onebyone(embedding_dim = embedding_dim, hidden_dim = hidden_dim, vocab_size = len(corpus),\\\n",
    "                       batch_size = batch_size, use_cuda = use_cuda, embeddings = embeddings)\n",
    "    print(model)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    data_dir = './ready_data_onebyone'\n",
    "    ftrain = 'mc500.train.tsv'\n",
    "    fdev = 'mc500.dev.tsv'\n",
    "\n",
    "    train_set = QA_Dataset(data_dir, ftrain, corpus, onebyone = True)\n",
    "    dev_set = QA_Dataset(data_dir, fdev, corpus, onebyone = True)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "    dev_loader = DataLoader(dev_set, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "    \n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = learning_rate, weight_decay=1e-2)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    train_loss_ = []\n",
    "    test_loss_ = []\n",
    "    train_acc_ = []\n",
    "    test_acc_ = []\n",
    "\n",
    "    rawQuestions, rawAnswers, rawLabels, rawOutput = [], [], [], []\n",
    "\n",
    "    # Training \n",
    "    for epoch in range(num_epochs):\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0\n",
    "        for iter, data in enumerate(train_loader):\n",
    "            questions, answers, labels, _, _, _ = data\n",
    "            labels = torch.squeeze(labels)\n",
    "            l = labels\n",
    "            if use_cuda:\n",
    "                q, l = Variable(questions.cuda()).t(), labels.cuda()\n",
    "                a = Variable(answers[0].cuda()).t()\n",
    "                b = Variable(answers[1].cuda()).t()\n",
    "                c = Variable(answers[2].cuda()).t()\n",
    "                d = Variable(answers[3].cuda()).t()\n",
    "            else:\n",
    "                q = Variable(questions).t()\n",
    "                a = Variable(answers[0]).t()\n",
    "                b = Variable(answers[1]).t()\n",
    "                c = Variable(answers[2]).t()\n",
    "                d = Variable(answers[3]).t()\n",
    "            model.batch_size = len(labels)\n",
    "            output = model(q, a, b, c, d)\n",
    "            loss = loss_function(output.cpu(), Variable(l).cpu())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculating training accuracy\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            total_acc += (pred.cpu() == labels).sum()\n",
    "            total += len(labels)\n",
    "            total_loss += loss.data[0]\n",
    "        train_loss_.append(total_loss / total)\n",
    "        train_acc_.append(total_acc / total)\n",
    "        if save_model:\n",
    "            fname = '{}/Epoch_{}.model'.format('./checkpoints', epoch)\n",
    "            save_checkpoint({'epoch': epoch + 1,\n",
    "                             'state_dict': model.state_dict(),\n",
    "                             'optimizer': optimizer.state_dict()},\n",
    "                           fname = fname)\n",
    "\n",
    "        # Testing/Validating\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0\n",
    "        for iter, data in enumerate(dev_loader):\n",
    "            questions, answers, labels, origQuestions, origAnswers, origLabels = data\n",
    "            labels = torch.squeeze(labels)\n",
    "            l = labels\n",
    "            if use_cuda:\n",
    "                q, l = Variable(questions.cuda()).t(), labels.cuda()\n",
    "                a = Variable(answers[0].cuda()).t()\n",
    "                b = Variable(answers[1].cuda()).t()\n",
    "                c = Variable(answers[2].cuda()).t()\n",
    "                d = Variable(answers[3].cuda()).t()\n",
    "            else:\n",
    "                q = Variable(questions).t()\n",
    "                a = Variable(answers[0]).t()\n",
    "                b = Variable(answers[1]).t()\n",
    "                c = Variable(answers[2]).t()\n",
    "                d = Variable(answers[3]).t()\n",
    "\n",
    "            model.batch_size = len(labels)\n",
    "            output = model(q, a, b, c, d)\n",
    "            loss = loss_function(output.cpu(), Variable(labels))\n",
    "            if epoch == num_epochs - 1:\n",
    "                rawQuestions.append(origQuestions)\n",
    "                rawAnswers.append(origAnswers)\n",
    "                rawLabels.append(origLabels)\n",
    "                rawOutput.append(output.data.cpu().numpy())\n",
    "\n",
    "            # Calculating training accuracy\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            total_acc += (pred.cpu() == labels).sum()\n",
    "            total += len(labels)\n",
    "            total_loss += loss.data[0]\n",
    "        test_loss_.append(total_loss / total)\n",
    "        test_acc_.append(total_acc / total)\n",
    "        \n",
    "        print(\"[Epoch %3d/%3d] Training loss: %.3f, Testing loss: %.3f, Training acc: %.3f, Testing acc: %.3f\"%\\\n",
    "              (epoch, num_epochs, train_loss_[epoch], test_loss_[epoch],\n",
    "                                 train_acc_[epoch], test_acc_[epoch]))\n",
    "\n",
    "    correct, wrong = [], []\n",
    "    for i in range(len(rawLabels)):\n",
    "        for j in range(len(rawOutput[i])):\n",
    "            # print(rawLabels[i][j], rawOutput[i][j])\n",
    "            choice = int(np.argmax(rawOutput[i][j]))\n",
    "            if rawLabels[i][j] == choice:\n",
    "                correct.append((rawQuestions[i][j], rawAnswers[i][j], rawLabels[i][j], choice))\n",
    "            else:\n",
    "                wrong.append((rawQuestions[i][j], rawAnswers[i][j], rawLabels[i][j], choice))\n",
    "    \n",
    "    with open(\"correct_500.json\", \"w\") as f:\n",
    "        json.dump(correct, f)\n",
    "\n",
    "    with open(\"wrong_500.json\", \"w\") as f:\n",
    "        json.dump(wrong, f)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_onebyone(\n",
      "  (word_embeddings): Embedding(5666, 300)\n",
      "  (lstm): LSTM(300, 64)\n",
      "  (hidden2label): Linear(in_features=128, out_features=1)\n",
      "  (softmax): Softmax()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WANG\\Downloads\\nn4nlp\\nn4nlp\\utils.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(torch.cat([qa, qb, qc, qd], 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch   0/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.211, Testing acc: 0.208\n",
      "[Epoch   1/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.221, Testing acc: 0.217\n",
      "[Epoch   2/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.229, Testing acc: 0.208\n",
      "[Epoch   3/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.264, Testing acc: 0.208\n",
      "[Epoch   4/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.271, Testing acc: 0.225\n",
      "[Epoch   5/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.293, Testing acc: 0.225\n",
      "[Epoch   6/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.293, Testing acc: 0.225\n",
      "[Epoch   7/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.304, Testing acc: 0.225\n",
      "[Epoch   8/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.307, Testing acc: 0.233\n",
      "[Epoch   9/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.307, Testing acc: 0.233\n",
      "[Epoch  10/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.304, Testing acc: 0.233\n",
      "[Epoch  11/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.304, Testing acc: 0.233\n",
      "[Epoch  12/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.304, Testing acc: 0.233\n",
      "[Epoch  13/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.300, Testing acc: 0.233\n",
      "[Epoch  14/ 20] Training loss: 0.089, Testing loss: 0.092, Training acc: 0.300, Testing acc: 0.233\n",
      "[Epoch  15/ 20] Training loss: 0.088, Testing loss: 0.092, Training acc: 0.300, Testing acc: 0.233\n",
      "[Epoch  16/ 20] Training loss: 0.088, Testing loss: 0.092, Training acc: 0.304, Testing acc: 0.233\n",
      "[Epoch  17/ 20] Training loss: 0.088, Testing loss: 0.092, Training acc: 0.307, Testing acc: 0.233\n",
      "[Epoch  18/ 20] Training loss: 0.088, Testing loss: 0.092, Training acc: 0.307, Testing acc: 0.233\n",
      "[Epoch  19/ 20] Training loss: 0.088, Testing loss: 0.092, Training acc: 0.311, Testing acc: 0.233\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import LSTM_basic, QA_Dataset, LSTM_onebyone\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "torch.manual_seed(11)\n",
    "corpus_path = './corpus.pkl'\n",
    "embedding_path = './pretrained_embed.npy'\n",
    "save_model = False\n",
    "embedding_dim = 300\n",
    "hidden_dim = 64\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 20\n",
    "\n",
    "def load_corpus(corpus_path):\n",
    "    with open(corpus_path, 'rb') as handle:\n",
    "        corpus = pickle.load(handle)\n",
    "    return corpus\n",
    "\n",
    "def load_embeddings(embed_path):\n",
    "    return np.load(embed_path)\n",
    "\n",
    "def save_checkpoint(state, fname):\n",
    "    print(\"Save model to %s\"%fname)\n",
    "    torch.save(state, fname)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = load_corpus(corpus_path)\n",
    "    embeddings = load_embeddings(embedding_path)\n",
    "    # word2id = {y:x for x, y in corpus.items()}\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    # use_cuda = False\n",
    "    model = LSTM_onebyone(embedding_dim = embedding_dim, hidden_dim = hidden_dim, vocab_size = len(corpus),\\\n",
    "                       batch_size = batch_size, use_cuda = use_cuda, embeddings = embeddings)\n",
    "    print(model)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    data_dir = './ready_data_onebyone'\n",
    "    ftrain = 'mc160.train.tsv'\n",
    "    fdev = 'mc160.dev.tsv'\n",
    "\n",
    "    train_set = QA_Dataset(data_dir, ftrain, corpus, onebyone = True)\n",
    "    dev_set = QA_Dataset(data_dir, fdev, corpus, onebyone = True)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "    dev_loader = DataLoader(dev_set, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "    \n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = learning_rate, weight_decay=1e-2)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    train_loss_ = []\n",
    "    test_loss_ = []\n",
    "    train_acc_ = []\n",
    "    test_acc_ = []\n",
    "\n",
    "    rawQuestions, rawAnswers, rawLabels, rawOutput = [], [], [], []\n",
    "\n",
    "    # Training \n",
    "    for epoch in range(num_epochs):\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0\n",
    "        for iter, data in enumerate(train_loader):\n",
    "            questions, answers, labels, _, _, _ = data\n",
    "            labels = torch.squeeze(labels)\n",
    "            l = labels\n",
    "            if use_cuda:\n",
    "                q, l = Variable(questions.cuda()).t(), labels.cuda()\n",
    "                a = Variable(answers[0].cuda()).t()\n",
    "                b = Variable(answers[1].cuda()).t()\n",
    "                c = Variable(answers[2].cuda()).t()\n",
    "                d = Variable(answers[3].cuda()).t()\n",
    "            else:\n",
    "                q = Variable(questions).t()\n",
    "                a = Variable(answers[0]).t()\n",
    "                b = Variable(answers[1]).t()\n",
    "                c = Variable(answers[2]).t()\n",
    "                d = Variable(answers[3]).t()\n",
    "            model.batch_size = len(labels)\n",
    "            output = model(q, a, b, c, d)\n",
    "            loss = loss_function(output.cpu(), Variable(l).cpu())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculating training accuracy\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            total_acc += (pred.cpu() == labels).sum()\n",
    "            total += len(labels)\n",
    "            total_loss += loss.data[0]\n",
    "        train_loss_.append(total_loss / total)\n",
    "        train_acc_.append(total_acc / total)\n",
    "        if save_model:\n",
    "            fname = '{}/Epoch_{}.model'.format('./checkpoints', epoch)\n",
    "            save_checkpoint({'epoch': epoch + 1,\n",
    "                             'state_dict': model.state_dict(),\n",
    "                             'optimizer': optimizer.state_dict()},\n",
    "                           fname = fname)\n",
    "\n",
    "        # Testing/Validating\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0\n",
    "        for iter, data in enumerate(dev_loader):\n",
    "            questions, answers, labels, origQuestions, origAnswers, origLabels = data\n",
    "            labels = torch.squeeze(labels)\n",
    "            l = labels\n",
    "            if use_cuda:\n",
    "                q, l = Variable(questions.cuda()).t(), labels.cuda()\n",
    "                a = Variable(answers[0].cuda()).t()\n",
    "                b = Variable(answers[1].cuda()).t()\n",
    "                c = Variable(answers[2].cuda()).t()\n",
    "                d = Variable(answers[3].cuda()).t()\n",
    "            else:\n",
    "                q = Variable(questions).t()\n",
    "                a = Variable(answers[0]).t()\n",
    "                b = Variable(answers[1]).t()\n",
    "                c = Variable(answers[2]).t()\n",
    "                d = Variable(answers[3]).t()\n",
    "\n",
    "            model.batch_size = len(labels)\n",
    "            output = model(q, a, b, c, d)\n",
    "            loss = loss_function(output.cpu(), Variable(labels))\n",
    "            if epoch == num_epochs - 1:\n",
    "                rawQuestions.append(origQuestions)\n",
    "                rawAnswers.append(origAnswers)\n",
    "                rawLabels.append(origLabels)\n",
    "                rawOutput.append(output.data.cpu().numpy())\n",
    "\n",
    "            # Calculating training accuracy\n",
    "            _, pred = torch.max(output.data, 1)\n",
    "            total_acc += (pred.cpu() == labels).sum()\n",
    "            total += len(labels)\n",
    "            total_loss += loss.data[0]\n",
    "        test_loss_.append(total_loss / total)\n",
    "        test_acc_.append(total_acc / total)\n",
    "        \n",
    "        print(\"[Epoch %3d/%3d] Training loss: %.3f, Testing loss: %.3f, Training acc: %.3f, Testing acc: %.3f\"%\\\n",
    "              (epoch, num_epochs, train_loss_[epoch], test_loss_[epoch],\n",
    "                                 train_acc_[epoch], test_acc_[epoch]))\n",
    "\n",
    "    correct, wrong = [], []\n",
    "    for i in range(len(rawLabels)):\n",
    "        for j in range(len(rawOutput[i])):\n",
    "            # print(rawLabels[i][j], rawOutput[i][j])\n",
    "            choice = int(np.argmax(rawOutput[i][j]))\n",
    "            if rawLabels[i][j] == choice:\n",
    "                correct.append((rawQuestions[i][j], rawAnswers[i][j], rawLabels[i][j], choice))\n",
    "            else:\n",
    "                wrong.append((rawQuestions[i][j], rawAnswers[i][j], rawLabels[i][j], choice))\n",
    "    \n",
    "    with open(\"correct_160.json\", \"w\") as f:\n",
    "        json.dump(correct, f)\n",
    "\n",
    "    with open(\"wrong_160.json\", \"w\") as f:\n",
    "        json.dump(wrong, f)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WANG\\Downloads\\nn4nlp\\nn4nlp\\utils.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(torch.cat([qa, qb, qc, qd], 1))\n"
     ]
    }
   ],
   "source": [
    "fdev = 'mc160.test.tsv'\n",
    "dev_set = QA_Dataset(data_dir, fdev, corpus, onebyone = True)\n",
    "\n",
    "dev_loader = DataLoader(dev_set, batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "rawQuestions, rawAnswers, rawLabels, rawOutput = [], [], [], []\n",
    "total_acc = 0.0\n",
    "total_loss = 0.0\n",
    "total = 0\n",
    "for iter, data in enumerate(dev_loader):\n",
    "    questions, answers, labels, origQuestions, origAnswers, origLabels = data\n",
    "    labels = torch.squeeze(labels)\n",
    "    l = labels\n",
    "    if use_cuda:\n",
    "        q, l = Variable(questions.cuda()).t(), labels.cuda()\n",
    "        a = Variable(answers[0].cuda()).t()\n",
    "        b = Variable(answers[1].cuda()).t()\n",
    "        c = Variable(answers[2].cuda()).t()\n",
    "        d = Variable(answers[3].cuda()).t()\n",
    "    else:\n",
    "        q = Variable(questions).t()\n",
    "        a = Variable(answers[0]).t()\n",
    "        b = Variable(answers[1]).t()\n",
    "        c = Variable(answers[2]).t()\n",
    "        d = Variable(answers[3]).t()\n",
    "\n",
    "    model.batch_size = len(labels)\n",
    "    output = model(q, a, b, c, d)\n",
    "    loss = loss_function(output.cpu(), Variable(labels))\n",
    "    if epoch == num_epochs - 1:\n",
    "        rawQuestions.append(origQuestions)\n",
    "        rawAnswers.append(origAnswers)\n",
    "        rawLabels.append(origLabels)\n",
    "        rawOutput.append(output.data.cpu().numpy())\n",
    "\n",
    "correct, wrong = [], []\n",
    "for i in range(len(rawLabels)):\n",
    "    for j in range(len(rawOutput[i])):\n",
    "        # print(rawLabels[i][j], rawOutput[i][j])\n",
    "        choice = int(np.argmax(rawOutput[i][j]))\n",
    "        if rawLabels[i][j] == choice:\n",
    "            correct.append((rawQuestions[i][j], rawAnswers[i][j], rawLabels[i][j], choice))\n",
    "        else:\n",
    "            wrong.append((rawQuestions[i][j], rawAnswers[i][j], rawLabels[i][j], choice))\n",
    "    \n",
    "with open(\"correct_160.json\", \"w\") as f:\n",
    "    json.dump(correct, f)\n",
    "\n",
    "with open(\"wrong_160.json\", \"w\") as f:\n",
    "    json.dump(wrong, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
